{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db583eee",
   "metadata": {},
   "source": [
    "# Predicting Subreddit Subjects: Exploring the Capabilities of Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f432c5e4",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "\"Despite the abundance of online forums and social media platforms, categorizing user-generated content remains a challenge. However, recent advances in machine learning have enabled the creation of models that can predict the subject of a subreddit based on the data from the subreddit itself. Despite the success of these models, there is a need to explore their limitations, accuracy, and potential applications in various domains. Here in T-Mobile as a data scientist, my focus is on trying diffrenet machine learning algorithms to find the best model for predicting two different subreddits of our devices (Apple Watch and Samsung Galaxy Watch) based on subreddit contents.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17a426d",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Online forums and social media platforms like Reddit have become valuable sources of information and feedback for businesses and individuals alike. With millions of users sharing their thoughts, opinions, and experiences on a wide range of topics, these platforms provide rich data that can be used to gain insights into customer needs and preferences. However, analyzing this data manually can be time-consuming and inefficient, especially when dealing with large volumes of posts and comments.\n",
    "\n",
    "In recent years, machine learning algorithms have emerged as powerful tools for analyzing and classifying text data. These algorithms can learn to identify patterns, relationships, and trends in large datasets and make predictions based on those patterns. One area where these algorithms could show great promise is in predicting the subreddit to which a post belongs based on its content.\n",
    "\n",
    "In this project, we present an approach for collecting and preprocessing the data from Apple Watch and Samsung Galaxy Watch subreddits and using natural language processing techniques to extract features from the text data. We will then train and test machine learning algorithms to determine the most accurate algorithm for predicting the subreddit based on the content of the post.\n",
    "\n",
    "By leveraging the insights from our analysis, we can gain a better understanding of the language used by users when discussing the Apple Watch and Samsung Galaxy Watch. This information can be used to inform our product development, marketing, and customer service strategies, helping to focus on products that better meet the needs and preferences of your customers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7984b29",
   "metadata": {},
   "source": [
    "## Contents:\n",
    "\n",
    "- [Problem Statement](#Problem-Statement)\n",
    "- [Background](#Background)\n",
    "- [Pushshift Reddit API](#Pushshift-Reddit-API)\n",
    "- [Imports](#Imports)\n",
    "- [Functions](#Functions)\n",
    "- [Fetch Data](#Fetch-Data)\n",
    "- [Save Datasets](#Save-Datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf625425",
   "metadata": {},
   "source": [
    "# Pushshift Reddit API\n",
    "\n",
    "The [pushshift.io](https://github.com/pushshift/api) Reddit API was designed and created by the /r/datasets mod team to help provide enhanced functionality and search capabilities for searching Reddit comments and submissions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1633205e",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0c1856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbbfb07",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4040c5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for fetching data from seubreddit posts and return a datafarme with the time of last post\n",
    "def fetch_data(url, subreddit, size, before=None):\n",
    "\n",
    "    # Define a parameters that will be used in our API\n",
    "    params = {\n",
    "        \n",
    "        # Subreddit name\n",
    "        'subreddit': subreddit,\n",
    "        \n",
    "        # Number of posts (pushif api can't get more that 500 posts in each iteration)\n",
    "        'size': size,\n",
    "        \n",
    "        # Before the specific time\n",
    "        'before': before\n",
    "    }\n",
    "    \n",
    "    # Send a Get Request to the specified URL with defined parameters\n",
    "    res = requests.get(url, params)\n",
    "    \n",
    "    # Extract the JSON data from the response\n",
    "    data = res.json()\n",
    "    \n",
    "    # Get posts out of data\n",
    "    posts = data['data']\n",
    "    \n",
    "    # Make a posts datframe\n",
    "    df = pd.DataFrame(posts)\n",
    "    \n",
    "    # Return the dataframe with the time of last post in that dataframe\n",
    "    return df, posts[-1]['created_utc']\n",
    "\n",
    "# Define a function for repeating above function in the number of iterations\n",
    "def fetch_multiple_data(url, subreddit, size, num_iterations):\n",
    "    dfs = []\n",
    "    before = None\n",
    "    \n",
    "    # For loop for iterating the process based on the number of num_iterations\n",
    "    for _ in range(num_iterations):\n",
    "        df, before = fetch_data(url, subreddit, size, before)\n",
    "        dfs.append(df)\n",
    "    \n",
    "    # Concatenate created dataframes\n",
    "    combined_df = pd.concat(dfs, axis=0)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cc3623",
   "metadata": {},
   "source": [
    "## Fetch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a01fce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 3,500 posts out of applewatch subrddit \n",
    "url = \"https://api.pushshift.io/reddit/search/submission\"\n",
    "subreddit = \"applewatch\"\n",
    "size = 500\n",
    "num_iterations = 7\n",
    "\n",
    "apple = fetch_multiple_data(url, subreddit, size, num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382bb3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 3,500 posts out of galaxywatch subreddit\n",
    "url = \"https://api.pushshift.io/reddit/search/submission\"\n",
    "subreddit = \"galaxywatch\"\n",
    "size = 500\n",
    "num_iterations = 7\n",
    "\n",
    "samsung = fetch_multiple_data(url, subreddit, size, num_iterations)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f554f0e3",
   "metadata": {},
   "source": [
    "## Save Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8f5574",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple.to_csv(\"../data/apple.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fe4ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "samsung.to_csv(\"../data/samsung.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
